{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In Part III, we will use machine learning techniques to predict 'Occupancy'. The process goes like this: \n",
    "\n",
    "![MachineLearningProcess](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/CommonAssets/MachineLearningProcess.png)\n",
    "\n",
    "We put this section on all of the projects in UpLevel so bear with us if you've seen this before. \n",
    "\n",
    "Generally, the machine learning process has five parts:\n",
    "1. <strong>Split your data into train and test set</strong>\n",
    "2. <strong>Model creation</strong>\n",
    "<br>\n",
    "Import your models from sklearn and instantiate them (assign model object to a variable)\n",
    "3. <strong>model fitting</strong>\n",
    "<br>\n",
    "Fit your training data into the model and train train train\n",
    "4. <strong>model prediction</strong>\n",
    "<br>\n",
    "Make a set of predictions using your test data, and\n",
    "5. <strong>Model assessment</strong>\n",
    "<br>\n",
    "Compare your predictions with ground truth in test data\n",
    "\n",
    "Highly recommended readings:\n",
    "1. [Important] https://scipy-lectures.org/packages/scikit-learn/index.html\n",
    "2. https://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/\n",
    "3. https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "### Step 1: Import your libraries\n",
    "We will be using models from sklearn - a popular machine learning library. However, we won't import everything from sklearn and take just what we need. \n",
    "\n",
    "We'll need to import plotting libraries to plot our predictions against the ground truth (test data). \n",
    "\n",
    "Import the following:\n",
    "- pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import your library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read the CSV from Part II as a DataFrame\n",
    "Read your CSV from the previous Part as a DataFrame. \n",
    "\n",
    "You should have:\n",
    "- 20,560 rows\n",
    "- 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-02 14:19:00</td>\n",
       "      <td>23.7000</td>\n",
       "      <td>26.2720</td>\n",
       "      <td>585.200000</td>\n",
       "      <td>749.200000</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-02 14:19:59</td>\n",
       "      <td>23.7180</td>\n",
       "      <td>26.2900</td>\n",
       "      <td>578.400000</td>\n",
       "      <td>760.400000</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-02 14:21:00</td>\n",
       "      <td>23.7300</td>\n",
       "      <td>26.2300</td>\n",
       "      <td>572.666667</td>\n",
       "      <td>769.666667</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-02 14:22:00</td>\n",
       "      <td>23.7225</td>\n",
       "      <td>26.1250</td>\n",
       "      <td>493.750000</td>\n",
       "      <td>774.750000</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-02 14:23:00</td>\n",
       "      <td>23.7540</td>\n",
       "      <td>26.2000</td>\n",
       "      <td>488.600000</td>\n",
       "      <td>779.000000</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20555</th>\n",
       "      <td>2015-02-18 09:15:00</td>\n",
       "      <td>20.8150</td>\n",
       "      <td>27.7175</td>\n",
       "      <td>429.750000</td>\n",
       "      <td>1505.250000</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20556</th>\n",
       "      <td>2015-02-18 09:16:00</td>\n",
       "      <td>20.8650</td>\n",
       "      <td>27.7450</td>\n",
       "      <td>423.500000</td>\n",
       "      <td>1514.500000</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20557</th>\n",
       "      <td>2015-02-18 09:16:59</td>\n",
       "      <td>20.8900</td>\n",
       "      <td>27.7450</td>\n",
       "      <td>423.500000</td>\n",
       "      <td>1521.500000</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20558</th>\n",
       "      <td>2015-02-18 09:17:59</td>\n",
       "      <td>20.8900</td>\n",
       "      <td>28.0225</td>\n",
       "      <td>418.750000</td>\n",
       "      <td>1632.000000</td>\n",
       "      <td>0.004279</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20559</th>\n",
       "      <td>2015-02-18 09:19:00</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>28.1000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>1864.000000</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20560 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date  Temperature  Humidity       Light          CO2  \\\n",
       "0      2015-02-02 14:19:00      23.7000   26.2720  585.200000   749.200000   \n",
       "1      2015-02-02 14:19:59      23.7180   26.2900  578.400000   760.400000   \n",
       "2      2015-02-02 14:21:00      23.7300   26.2300  572.666667   769.666667   \n",
       "3      2015-02-02 14:22:00      23.7225   26.1250  493.750000   774.750000   \n",
       "4      2015-02-02 14:23:00      23.7540   26.2000  488.600000   779.000000   \n",
       "...                    ...          ...       ...         ...          ...   \n",
       "20555  2015-02-18 09:15:00      20.8150   27.7175  429.750000  1505.250000   \n",
       "20556  2015-02-18 09:16:00      20.8650   27.7450  423.500000  1514.500000   \n",
       "20557  2015-02-18 09:16:59      20.8900   27.7450  423.500000  1521.500000   \n",
       "20558  2015-02-18 09:17:59      20.8900   28.0225  418.750000  1632.000000   \n",
       "20559  2015-02-18 09:19:00      21.0000   28.1000  409.000000  1864.000000   \n",
       "\n",
       "       HumidityRatio  Occupancy  weekday  hour  minute  \n",
       "0           0.004764          1        0    14      19  \n",
       "1           0.004773          1        0    14      19  \n",
       "2           0.004765          1        0    14      21  \n",
       "3           0.004744          1        0    14      22  \n",
       "4           0.004767          1        0    14      23  \n",
       "...              ...        ...      ...   ...     ...  \n",
       "20555       0.004213          1        2     9      15  \n",
       "20556       0.004230          1        2     9      16  \n",
       "20557       0.004237          1        2     9      16  \n",
       "20558       0.004279          1        2     9      17  \n",
       "20559       0.004321          1        2     9      19  \n",
       "\n",
       "[20560 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Read the CSV from Part II\n",
    "df = pd.read_csv(\"df_comb.csv\", index_col = 0)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare your independent and dependent variables\n",
    "At this point, let's prepare our indepedent and dependent variables. \n",
    "\n",
    "1. Declare a variable, and assign your independent variables to it by dropping 'date' and 'Occupancy'\n",
    "2. Declare another variable, and assign only values 'Occupancy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare your independent and dependent variables\n",
    "X = df.drop(['date', 'Occupancy'], axis = 1)\n",
    "y = df['Occupancy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import machine learning libraries\n",
    "Time to import other libraries.\n",
    "\n",
    "The resources provided at the top of this notebook will be immensely useful if you're new to modelling. \n",
    "\n",
    "Import the following libraries and methods:\n",
    "1. train_test_split - sklearn.model_selection\n",
    "2. DummyClassifier - sklearn.dummy\n",
    "3. LogisticRegression - sklearn.linear_model\n",
    "4. DecisionTreeClassifier - sklearn.tree\n",
    "5. RandomForestClassifier - sklearn.ensemble\n",
    "6. f1_score - sklearn.metrics\n",
    "7. confusion_matrix - sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Import the machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Split your dataset into train and test\n",
    "Now that you have finished importing the libraries you need, split the dataset into train and test at a 80/20 split.\n",
    "\n",
    "Don't forget to stratify by your dependent values with the stratify parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split your dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Train a DummyClassifier\n",
    "This is what you'll need to do:\n",
    "1. Start with a model\n",
    "2. Declare a variable, and store your model in it (don't forget to use brackets)\n",
    "3. Fit your training data into the instantiated model\n",
    "4. Declare a variable that contains predictions from the model you just trained, using the train dataset (X_test)\n",
    "5. Compare the prediction with the actual result (y_test) with the f1_score\n",
    "6. Plot a confusion_matrix using the prediction (y-axis) vs actual y_test (x-axis) \n",
    "\n",
    "The recommended readings will be very helpful.\n",
    "\n",
    "Let's start with the DummyClassifier to establish a baseline. This will be useful as we train other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3162,    0],\n",
       "       [ 950,    0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6a: Declare a variable to store the model\n",
    "dummy = DummyClassifier()\n",
    "\n",
    "# Step 6b: Fit your train dataset\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "# Step 6c: Declare a variable and store your predictions that you make with your model using X test data\n",
    "dummy_pred = dummy.predict(X_test)\n",
    "\n",
    "# Step 6d: Print the f1_score between the y test and dummy prediction\n",
    "print(\"F1-score:\", metrics.f1_score(y_test, dummy_pred))\n",
    "\n",
    "# Step 6e: Print a confusion_matrix between y_test and your prediction\n",
    "confusion_matrix(y_test, dummy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train a LogisticRegression\n",
    "Now that we have established the baseline performance of a classifier, let's train a LogisticRegression model. \n",
    "\n",
    "Similar to how we did in training the DummyClassifier, train the model and then assess the model performance with the f1_score and the confusion_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.9772492244053775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelly/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3123,   39],\n",
       "       [   5,  945]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7a: Declare a variable to store the LogisticRegression model\n",
    "logr = LogisticRegression()\n",
    "\n",
    "# Step 7b: Fit your train dataset\n",
    "logr.fit(X_train, y_train)\n",
    "\n",
    "# Step 7c: Declare a variable and store your predictions that you make with your model using X test data \n",
    "logr_pred = logr.predict(X_test)\n",
    "\n",
    "# Step 7d: Print f1_score between the y test and LogisticRegression prediction\n",
    "print(\"F1-score:\", metrics.f1_score(y_test, logr_pred))\n",
    "\n",
    "# Step 7e: Print a confusion_matrix between y_test and your prediction\n",
    "confusion_matrix(y_test, logr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Train a DecisionTreeClassifier\n",
    "The LogisticRegression model should perform quite impressively, based on the confusion matrix and the f1_score. \n",
    "\n",
    "Can we improve it further? Let's find out by training and assessing a DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.9837270341207349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3144,   18],\n",
       "       [  13,  937]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Train a DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "print(\"F1-score:\", metrics.f1_score(y_test, tree_pred))\n",
    "confusion_matrix(y_test, tree_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Train a RandomForestClassifier\n",
    "The DecisionTreeClassifier is most likely (slightly) better than the LogisticRegression results, in terms of f1 score.\n",
    "\n",
    "Train a RandomForestClassifier and see if you can push the performance even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.9868904037755636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3146,   16],\n",
       "       [   9,  941]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 9: Train a RandomForestClassifier\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train, y_train)\n",
    "forest_pred = forest.predict(X_test)\n",
    "print(\"F1-score:\", metrics.f1_score(y_test, forest_pred))\n",
    "confusion_matrix(y_test, forest_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Train other classifiers\n",
    "There are a few other classifiers that you can try, apart from the three that we used above.\n",
    "\n",
    "It's hard to top RandomForestClassifier for this dataset, but it's still worth typing it out to get some practice in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Get a feature importances DataFrame\n",
    "Create a DataFrame containing the feature importances of your best performing model. \n",
    "\n",
    "For example, this is what an example DataFrame would look like:\n",
    "\n",
    "![RandomForestClassifierFeatureImportances](https://uplevelsg.s3.ap-southeast-1.amazonaws.com/ProjectRoomOccupancy/RandomForestClassifierFeatureImportances.png)\n",
    "\n",
    "What's the most important feature? \n",
    "\n",
    "Does it align with what you observed in Part II? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>0.161425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Humidity</td>\n",
       "      <td>0.017774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Light</td>\n",
       "      <td>0.544876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CO2</td>\n",
       "      <td>0.102417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HumidityRatio</td>\n",
       "      <td>0.023608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weekday</td>\n",
       "      <td>0.050829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hour</td>\n",
       "      <td>0.090849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>minute</td>\n",
       "      <td>0.008222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature  importance\n",
       "0    Temperature    0.161425\n",
       "1       Humidity    0.017774\n",
       "2          Light    0.544876\n",
       "3            CO2    0.102417\n",
       "4  HumidityRatio    0.023608\n",
       "5        weekday    0.050829\n",
       "6           hour    0.090849\n",
       "7         minute    0.008222"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 10: Create a DataFrame containing feature importances\n",
    "forest.feature_importances_\n",
    "\n",
    "pd.DataFrame({'feature': X.columns,\n",
    "             'importance': forest.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling without 'Light'\n",
    "Whichever model you used, it's most likely that you identified \"Light\" as the most important feature in the model.\n",
    "\n",
    "This makes sense, because if there's 'Light', it's most likely that there's someone in the room. \n",
    "\n",
    "Here's a challenge - let's try modelling without \"Light\" as a feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Repeat Step 3 and drop 'Light'\n",
    "Repeat what you did in Step 3, i.e. prepare independent and dependent values.\n",
    "\n",
    "However, this time drop 'Light' on top of 'date' and 'Occupancy' to prepare your independent values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Prepare new independent and dependent values\n",
    "X = df.drop(['date', 'Occupancy', 'Light'], axis = 1)\n",
    "y = df['Occupancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Repeat Steps 6-10\n",
    "Now that you've removed 'Light' column, time to split your data and model again.\n",
    "\n",
    "One thing to note - when you train a LogisticRegression you <strong>may</strong> receive a warning. Don't worry - just increase the value of max_iter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12a: Split your data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3162,    0],\n",
       "       [ 950,    0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 12b: Train and assess a DummyClassifier\n",
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy_pred = dummy.predict(X_test)\n",
    "print(\"F1-score:\", metrics.f1_score(y_test, dummy_pred))\n",
    "confusion_matrix(y_test, dummy_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.4866920152091255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelly/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2918,  244],\n",
       "       [ 566,  384]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 12b: Train and assess a LogisticRegression\n",
    "logr = LogisticRegression()\n",
    "logr.fit(X_train, y_train)\n",
    "logr_pred = logr.predict(X_test)\n",
    "print(\"F1-score:\", metrics.f1_score(y_test, logr_pred))\n",
    "confusion_matrix(y_test, logr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.9763531266421439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3138,   24],\n",
       "       [  21,  929]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 12c: Train a DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "print(\"F1-score:\", metrics.f1_score(y_test, tree_pred))\n",
    "confusion_matrix(y_test, tree_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.9837781266352694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3141,   21],\n",
       "       [  10,  940]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 12d: Train a RandomForestClassifier\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train, y_train)\n",
    "forest_pred = forest.predict(X_test)\n",
    "print(\"F1-score:\", metrics.f1_score(y_test, forest_pred))\n",
    "confusion_matrix(y_test, forest_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temperature</td>\n",
       "      <td>0.281392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Humidity</td>\n",
       "      <td>0.065274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CO2</td>\n",
       "      <td>0.228614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HumidityRatio</td>\n",
       "      <td>0.068114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekday</td>\n",
       "      <td>0.089710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hour</td>\n",
       "      <td>0.241695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>minute</td>\n",
       "      <td>0.025201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature  importance\n",
       "0    Temperature    0.281392\n",
       "1       Humidity    0.065274\n",
       "2            CO2    0.228614\n",
       "3  HumidityRatio    0.068114\n",
       "4        weekday    0.089710\n",
       "5           hour    0.241695\n",
       "6         minute    0.025201"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 12e: Create a DataFrame containing feature importances\n",
    "pd.DataFrame({'feature': X.columns,\n",
    "             'importance': forest.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>Did removing 'Light' affect model performance adversely?</strong></summary>\n",
    "    <div>No, not really. The f1 score and confusion matrix look great</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>What were the features that were important?</strong></summary>\n",
    "    <div>In the new DataFrame, the features were 'Temperature', 'CO2', and 'hour'. It seems that the model considered these three features in the absence of light.</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end\n",
    "You did it! You've arrived at the end. Congratulations and well done on completing this project series! \n",
    "\n",
    "Let's review.\n",
    "1. In Part I, you collected the datasets and combined them to form a single DataFrame. You also investigated the data briefly to see if there was anything remarkable about it\n",
    "2. In Part II, you performed exploratory data analysis on the dataset, investigating distributions and relationships found between features. You also engineered additional features from the dataset for model building\n",
    "5. In Part III, you trained a machine learning model that can predict room occupancy based on sensor data. In addition, you modelled the problem without a major feature to see if the model performed equally well\n",
    "\n",
    "Go on, give yourself a pat on the back. We hope this project series has give you more confidence in coding and machine learning. \n",
    "\n",
    "Whatever you learn here is but a tip of the iceberg, and launchpad for bigger and better things to come. Come join us in our Telegram community over at https://bit.ly/UpLevelSG and our Facebook page at https://fb.com/UpLevelSG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
